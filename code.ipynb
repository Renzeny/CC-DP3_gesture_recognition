{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project CC-DP3 \n",
    "\n",
    "In this notebook we will show how gesture control can be used for sending commands to an agent, the agent will eventually execute an order related to this gesture.\n",
    "\n",
    "In this example we use an external webcam to record the livestream of given gestures and will display an animation in real-time to confirm the action related to the gesture.\n",
    "\n",
    "The gestures are as follows: \n",
    "\n",
    "    - FOLLOW: ðŸ‘†\n",
    "    - STOP: âœ‹\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def download_file(url, output_filename):\n",
    "    if platform.system() == \"Windows\":\n",
    "        # Windows: Use Invoke-WebRequest\n",
    "        powershell_command = f'Invoke-WebRequest -Uri \"{url}\" -OutFile \"{output_filename}\"'\n",
    "        subprocess.run([\"powershell\", \"-Command\", powershell_command], shell=True)\n",
    "    else:\n",
    "        # Non-Windows: Use wget\n",
    "        subprocess.run([\"wget\", \"-q\", url, \"-O\", output_filename])\n",
    "\n",
    "# This will download the model that will recognize the gestures - this model supports 7 hand gestures: ðŸ‘, ðŸ‘Ž, âœŒï¸, â˜ï¸, âœŠ, ðŸ‘‹, ðŸ¤Ÿ\n",
    "download_file(\"https://storage.googleapis.com/mediapipe-models/gesture_recognizer/gesture_recognizer/float16/1/gesture_recognizer.task\", \"gesture_recognizer.task\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mediapipe in /opt/homebrew/lib/python3.11/site-packages (0.10.8)\n",
      "Requirement already satisfied: absl-py in /opt/homebrew/lib/python3.11/site-packages (from mediapipe) (2.0.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /opt/homebrew/lib/python3.11/site-packages (from mediapipe) (23.1.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /opt/homebrew/lib/python3.11/site-packages (from mediapipe) (23.5.26)\n",
      "Requirement already satisfied: matplotlib in /opt/homebrew/lib/python3.11/site-packages (from mediapipe) (3.8.2)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/lib/python3.11/site-packages (from mediapipe) (1.26.2)\n",
      "Requirement already satisfied: opencv-contrib-python in /opt/homebrew/lib/python3.11/site-packages (from mediapipe) (4.8.1.78)\n",
      "Requirement already satisfied: protobuf<4,>=3.11 in /opt/homebrew/lib/python3.11/site-packages (from mediapipe) (3.20.3)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in /opt/homebrew/lib/python3.11/site-packages (from mediapipe) (0.4.6)\n",
      "Requirement already satisfied: CFFI>=1.0 in /opt/homebrew/lib/python3.11/site-packages (from sounddevice>=0.4.4->mediapipe) (1.16.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib->mediapipe) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib->mediapipe) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib->mediapipe) (4.44.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib->mediapipe) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/philippe/Library/Python/3.11/lib/python/site-packages (from matplotlib->mediapipe) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib->mediapipe) (10.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib->mediapipe) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/philippe/Library/Python/3.11/lib/python/site-packages (from matplotlib->mediapipe) (2.8.2)\n",
      "Requirement already satisfied: pycparser in /opt/homebrew/lib/python3.11/site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\n",
      "Requirement already satisfied: six>=1.5 in /Users/philippe/Library/Python/3.11/lib/python/site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: opencv-python in /opt/homebrew/lib/python3.11/site-packages (4.8.1.78)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /opt/homebrew/lib/python3.11/site-packages (from opencv-python) (1.26.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: imageio in /opt/homebrew/lib/python3.11/site-packages (2.32.0)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/lib/python3.11/site-packages (from imageio) (1.26.2)\n",
      "Requirement already satisfied: pillow<10.1.0,>=8.3.2 in /opt/homebrew/lib/python3.11/site-packages (from imageio) (10.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install mediapipe\n",
    "%pip install opencv-python\n",
    "%pip install imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1700601611.773235       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M1 Pro\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "I0000 00:00:1700601611.777200       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M1 Pro\n",
      "W0000 00:00:1700601611.777522       1 gesture_recognizer_graph.cc:129] Hand Gesture Recognizer contains CPU only ops. Sets HandGestureRecognizerGraph acceleration to Xnnpack.\n",
      "I0000 00:00:1700601611.778514       1 hand_gesture_recognizer_graph.cc:250] Custom gesture classifier is not defined.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/philippe/Desktop/code_Yannick/CC-DP3_gesture_recognition/gesture_recognizer.task\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import cv2\n",
    "import os\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import imageio\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "\n",
    "# Helper function for visualizing gesture recognition results\n",
    "def visualize_results(frame, gesture_results, verbose):\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "    if gesture_results:\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        annotated_image = copy.deepcopy(frame_rgb)\n",
    "\n",
    "        if verbose:\n",
    "            for hand_landmarks in gesture_results.hand_landmarks:\n",
    "                hand_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "                hand_landmarks_proto.landmark.extend([\n",
    "                    landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in hand_landmarks\n",
    "                ])\n",
    "\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    annotated_image,\n",
    "                    hand_landmarks_proto,\n",
    "                    mp.solutions.hands.HAND_CONNECTIONS,\n",
    "                    mp.solutions.drawing_styles.get_default_hand_landmarks_style(),\n",
    "                    mp.solutions.drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "            if assured_recognized_gesture:\n",
    "                cv2.rectangle(annotated_image, (0, 0), (400, 50), (0, 0, 0), -1)  # Black background\n",
    "                cv2.putText(annotated_image, f\"Recognized: {assured_recognized_gesture}\", (10, 30),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)  # White text\n",
    "\n",
    "        cv2.imshow(\"Webcam Feed\", cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "gif_A = \"Follow.gif\"\n",
    "gif_B = \"Stop.gif\"\n",
    "\n",
    "# verbose flag will show overlay on webcam footage\n",
    "VERBOSE = True\n",
    "\n",
    "# duration of the gif & timeout to pause hand gesture recognition\n",
    "DURATION = 2\n",
    "\n",
    "# Initialize MediaPipe for gesture recognition\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands()\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Load the gesture recognition model\n",
    "try:\n",
    "    # Load the gesture recognition model\n",
    "    # Get the current working directory\n",
    "    current_dir = os.getcwd()\n",
    "    model_path = os.path.join(current_dir, 'gesture_recognizer.task')\n",
    "    print(model_path)\n",
    "    base_options = python.BaseOptions(model_asset_path=model_path)\n",
    "    options = vision.GestureRecognizerOptions(base_options=base_options)\n",
    "    recognizer = vision.GestureRecognizer.create_from_options(options)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading the model: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Open the webcam (use 0 for the default camera)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Check if the webcam is opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open webcam.\")\n",
    "    exit()\n",
    "\n",
    "# Create a window to display the camera feed\n",
    "cv2.namedWindow(\"Webcam Feed\", cv2.WINDOW_NORMAL)\n",
    "\n",
    "# Flag to control the pause state and determine which gesture was recognized\n",
    "pause_time = None\n",
    "recognized_gesture = None\n",
    "assured_recognized_gesture = None\n",
    "gif_window_open = False\n",
    "gif_reader = None\n",
    "gif_frames = []\n",
    "\n",
    "# Storing recognized gestures in a stack\n",
    "gesture_stack = []\n",
    "\n",
    "# Threshold for setting assured recognized gesture\n",
    "threshold_gestures = 14  # 70% of 20\n",
    "\n",
    "while True:\n",
    "    # Read a frame from the webcam\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Check if the frame was read successfully\n",
    "    if not ret:\n",
    "        print(\"Error: Failed to grab frame.\")\n",
    "        break\n",
    "\n",
    "    # Check if it's time to pause analysis\n",
    "    if pause_time and time.time() - pause_time < DURATION:\n",
    "        # Display a specific GIF based on recognized gesture\n",
    "        if assured_recognized_gesture == \"Pointing_Up\" and gif_A:  # Checking if the GIF path is not empty\n",
    "            if not gif_window_open:\n",
    "                gif_frames = imageio.get_reader(gif_A)\n",
    "                gif_window_open = True\n",
    "\n",
    "            try:\n",
    "                gif_frame = gif_frames.get_next_data()\n",
    "                cv2.imshow(\"GIF Window\", cv2.cvtColor(gif_frame, cv2.COLOR_RGB2BGR))\n",
    "            except Exception as e:\n",
    "                gif_window_open = False\n",
    "                gif_frames = None\n",
    "\n",
    "        elif assured_recognized_gesture == \"Open_Palm\" and gif_B:  # Checking if the GIF path is not empty\n",
    "            if not gif_window_open:\n",
    "                gif_frames = imageio.get_reader(gif_B)\n",
    "                gif_window_open = True\n",
    "\n",
    "            try:\n",
    "                gif_frame = gif_frames.get_next_data()\n",
    "                cv2.imshow(\"GIF Window\", cv2.cvtColor(gif_frame, cv2.COLOR_RGB2BGR))\n",
    "            except Exception as e:\n",
    "                gif_window_open = False\n",
    "                gif_frames = None\n",
    "\n",
    "    # Close GIF window after duration\n",
    "    if gif_window_open and time.time() - pause_time >= DURATION:\n",
    "        cv2.destroyWindow(\"GIF Window\")\n",
    "        gif_window_open = False\n",
    "        gif_frames = None\n",
    "        pause_time = None\n",
    "        gesture_stack = []\n",
    "        assured_recognized_gesture = None\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    # Convert the frame received from OpenCV to a MediaPipeâ€™s Image object.\n",
    "    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame)\n",
    "\n",
    "    # Perform gesture recognition on the frame\n",
    "    gesture_results = recognizer.recognize(mp_image)\n",
    "\n",
    "    # Check for specific gestures to pause\n",
    "    if gesture_results:\n",
    "        if gesture_results.gestures != []:\n",
    "            recognized_gesture = gesture_results.gestures[0][0].category_name\n",
    "\n",
    "            # Add recognized gesture to the stack\n",
    "            gesture_stack.append(recognized_gesture)\n",
    "\n",
    "            # Maintain the stack size to 20 elements\n",
    "            if len(gesture_stack) > 20:\n",
    "                gesture_stack.pop(0)\n",
    "            \n",
    "            if len(gesture_stack) == 20:\n",
    "                # Check for the majority recognized gesture\n",
    "                gesture_counts = {gesture: gesture_stack.count(gesture) for gesture in gesture_stack}\n",
    "                max_count = max(gesture_counts.values())\n",
    "                majority_gesture = [gesture for gesture, count in gesture_counts.items() if count == max_count]\n",
    "\n",
    "                # Check if the majority gesture exceeds the threshold\n",
    "                if (max_count / 20) * 100 >= threshold_gestures:\n",
    "                    assured_recognized_gesture = majority_gesture[0]\n",
    "                    pause_time = time.time()\n",
    "\n",
    "    # Display the frame with gesture overlay in the window\n",
    "    visualize_results(frame, gesture_results, VERBOSE)\n",
    "\n",
    "    # Exit the loop if 'q' key is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        # Release the webcam\n",
    "        cap.release() \n",
    "        # Exit all windows\n",
    "        cv2.destroyAllWindows()\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

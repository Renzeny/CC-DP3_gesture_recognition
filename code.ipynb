{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project CC-DP3 \n",
    "\n",
    "In this notebook we will show how gesture control can be used for sending commands to an agent, the agent will eventually execute an order related to this gesture.\n",
    "\n",
    "In this example we use an external webcam to record the livestream of given gestures and will display an animation in real-time to confirm the action related to the gesture.\n",
    "\n",
    "The gestures are as follows: \n",
    "\n",
    "    - FOLLOW: ðŸ‘†\n",
    "    - STOP: âœ‹\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will download the model that will recognize the gestures - this model supports 7 hand gestures: ðŸ‘, ðŸ‘Ž, âœŒï¸, â˜ï¸, âœŠ, ðŸ‘‹, ðŸ¤Ÿ\n",
    "!wget -q https://storage.googleapis.com/mediapipe-models/gesture_recognizer/gesture_recognizer/float16/1/gesture_recognizer.task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: combining -O with -r or -p will mean that all downloaded content\n",
      "will be placed in the single file you specified.\n",
      "\n",
      "--2023-11-18 19:09:43--  https://media.tenor.com/1OMWOWwx98EAAAAC/upright-point.gif\n",
      "Resolving media.tenor.com (media.tenor.com)... 2a00:1450:400e:80e::200a, 2a00:1450:400e:800::200a, 2a00:1450:400e:810::200a, ...\n",
      "Connecting to media.tenor.com (media.tenor.com)|2a00:1450:400e:80e::200a|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1894549 (1,8M) [image/gif]\n",
      "Saving to: â€˜gif_A.gifâ€™\n",
      "\n",
      "gif_A.gif           100%[===================>]   1,81M  2,85MB/s    in 0,6s    \n",
      "\n",
      "2023-11-18 19:09:44 (2,85 MB/s) - â€˜gif_A.gifâ€™ saved [1894549/1894549]\n",
      "\n",
      "FINISHED --2023-11-18 19:09:44--\n",
      "Total wall clock time: 0,7s\n",
      "Downloaded: 1 files, 1,8M in 0,6s (2,85 MB/s)\n",
      "WARNING: combining -O with -r or -p will mean that all downloaded content\n",
      "will be placed in the single file you specified.\n",
      "\n",
      "--2023-11-18 19:09:44--  https://gifdb.com/images/high/michael-scott-no-hand-gesture-p7iyykb88vqq4ks2.gif\n",
      "Resolving gifdb.com (gifdb.com)... 2400:52e0:1e01::879:1, 84.17.46.53\n",
      "Connecting to gifdb.com (gifdb.com)|2400:52e0:1e01::879:1|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 7896598 (7,5M) [image/gif]\n",
      "Saving to: â€˜gif_B.gifâ€™\n",
      "\n",
      "gif_B.gif           100%[===================>]   7,53M  3,39MB/s    in 2,2s    \n",
      "\n",
      "2023-11-18 19:09:46 (3,39 MB/s) - â€˜gif_B.gifâ€™ saved [7896598/7896598]\n",
      "\n",
      "FINISHED --2023-11-18 19:09:46--\n",
      "Total wall clock time: 2,3s\n",
      "Downloaded: 1 files, 7,5M in 2,2s (3,39 MB/s)\n"
     ]
    }
   ],
   "source": [
    "# getting the GIFs for the example\n",
    "!wget -A .gif -r -l 1 -H -O gif_A.gif https://media.tenor.com/1OMWOWwx98EAAAAC/upright-point.gif\n",
    "!wget -A .gif -r -l 1 -H -O gif_B.gif https://gifdb.com/images/high/michael-scott-no-hand-gesture-p7iyykb88vqq4ks2.gif\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Invalid requirement: 'mediapipe,'\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Collecting opencv-python\n",
      "  Using cached opencv_python-4.8.1.78-cp37-abi3-macosx_11_0_arm64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /opt/homebrew/lib/python3.11/site-packages (from opencv-python) (1.26.2)\n",
      "Using cached opencv_python-4.8.1.78-cp37-abi3-macosx_11_0_arm64.whl (33.1 MB)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.8.1.78\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: imageio in /opt/homebrew/lib/python3.11/site-packages (2.32.0)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/lib/python3.11/site-packages (from imageio) (1.26.2)\n",
      "Requirement already satisfied: pillow<10.1.0,>=8.3.2 in /opt/homebrew/lib/python3.11/site-packages (from imageio) (10.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install mediapipe\n",
    "%pip install opencv-python\n",
    "%pip install imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1700427338.008054       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M1 Pro\n",
      "I0000 00:00:1700427338.012042       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M1 Pro\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1700427338.017483       1 gesture_recognizer_graph.cc:129] Hand Gesture Recognizer contains CPU only ops. Sets HandGestureRecognizerGraph acceleration to Xnnpack.\n",
      "I0000 00:00:1700427338.022273       1 hand_gesture_recognizer_graph.cc:250] Custom gesture classifier is not defined.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import imageio\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "\n",
    "# Helper function for visualizing gesture recognition results\n",
    "def visualize_results(frame, gesture_results, verbose):\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "    if gesture_results:\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        annotated_image = copy.deepcopy(frame_rgb)\n",
    "\n",
    "        if verbose:\n",
    "            for hand_landmarks in gesture_results.hand_landmarks:\n",
    "                hand_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "                hand_landmarks_proto.landmark.extend([\n",
    "                    landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in hand_landmarks\n",
    "                ])\n",
    "\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    annotated_image,\n",
    "                    hand_landmarks_proto,\n",
    "                    mp.solutions.hands.HAND_CONNECTIONS,\n",
    "                    mp.solutions.drawing_styles.get_default_hand_landmarks_style(),\n",
    "                    mp.solutions.drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "            if assured_recognized_gesture:\n",
    "                cv2.rectangle(annotated_image, (0, 0), (400, 50), (0, 0, 0), -1)  # Black background\n",
    "                cv2.putText(annotated_image, f\"Recognized: {assured_recognized_gesture}\", (10, 30),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)  # White text\n",
    "\n",
    "        cv2.imshow(\"Webcam Feed\", cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "# TODO: Add paths to GIFs here\n",
    "gif_A = \"gif_A.gif\"\n",
    "gif_B = \"gif_B.gif\"\n",
    "\n",
    "# verbose flag will show overlay on webcam footage\n",
    "VERBOSE = True\n",
    "\n",
    "# duration of the gif & timeout to pause hand gesture recognition\n",
    "DURATION = 2\n",
    "\n",
    "# Initialize MediaPipe for gesture recognition\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands()\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Load the gesture recognition model\n",
    "model_path = 'gesture_recognizer.task'\n",
    "base_options = python.BaseOptions(model_asset_path=model_path)\n",
    "options = vision.GestureRecognizerOptions(base_options=base_options)\n",
    "recognizer = vision.GestureRecognizer.create_from_options(options)\n",
    "\n",
    "# Open the webcam (use 0 for the default camera)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Check if the webcam is opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open webcam.\")\n",
    "    exit()\n",
    "\n",
    "# Create a window to display the camera feed\n",
    "cv2.namedWindow(\"Webcam Feed\", cv2.WINDOW_NORMAL)\n",
    "\n",
    "# Flag to control the pause state and determine which gesture was recognized\n",
    "pause_time = None\n",
    "recognized_gesture = None\n",
    "assured_recognized_gesture = None\n",
    "gif_window_open = False\n",
    "gif_reader = None\n",
    "gif_frames = []\n",
    "\n",
    "# Storing recognized gestures in a stack\n",
    "gesture_stack = []\n",
    "\n",
    "# Threshold for setting assured recognized gesture\n",
    "threshold_gestures = 14  # 70% of 20\n",
    "\n",
    "while True:\n",
    "    timestamp_ms = int(time.time() * 1000)  # Get timestamp in milliseconds\n",
    "\n",
    "    # Read a frame from the webcam\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Check if the frame was read successfully\n",
    "    if not ret:\n",
    "        print(\"Error: Failed to grab frame.\")\n",
    "        break\n",
    "\n",
    "    # Check if it's time to pause analysis\n",
    "    if pause_time and time.time() - pause_time < DURATION:\n",
    "        # Display a specific GIF based on recognized gesture\n",
    "        if assured_recognized_gesture == \"Pointing_Up\" and gif_A:  # Checking if the GIF path is not empty\n",
    "            if not gif_window_open:\n",
    "                gif_frames = imageio.get_reader(gif_A)\n",
    "                gif_window_open = True\n",
    "\n",
    "            try:\n",
    "                gif_frame = gif_frames.get_next_data()\n",
    "                cv2.imshow(\"GIF Window\", cv2.cvtColor(gif_frame, cv2.COLOR_RGB2BGR))\n",
    "            except Exception as e:\n",
    "                gif_window_open = False\n",
    "                gif_frames = None\n",
    "\n",
    "        elif assured_recognized_gesture == \"Open_Palm\" and gif_B:  # Checking if the GIF path is not empty\n",
    "            if not gif_window_open:\n",
    "                gif_frames = imageio.get_reader(gif_B)\n",
    "                gif_window_open = True\n",
    "\n",
    "            try:\n",
    "                gif_frame = gif_frames.get_next_data()\n",
    "                cv2.imshow(\"GIF Window\", cv2.cvtColor(gif_frame, cv2.COLOR_RGB2BGR))\n",
    "            except Exception as e:\n",
    "                gif_window_open = False\n",
    "                gif_frames = None\n",
    "\n",
    "    # Close GIF window after duration\n",
    "    if gif_window_open and time.time() - pause_time >= DURATION:\n",
    "        cv2.destroyWindow(\"GIF Window\")\n",
    "        gif_window_open = False\n",
    "        gif_frames = None\n",
    "        pause_time = None\n",
    "        gesture_stack = []\n",
    "        assured_recognized_gesture = None\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    # Convert the frame received from OpenCV to a MediaPipeâ€™s Image object.\n",
    "    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame)\n",
    "\n",
    "    # Perform gesture recognition on the frame\n",
    "    gesture_results = recognizer.recognize(mp_image)\n",
    "\n",
    "    # Check for specific gestures to pause\n",
    "    if gesture_results:\n",
    "        if gesture_results.gestures != []:\n",
    "            recognized_gesture = gesture_results.gestures[0][0].category_name\n",
    "\n",
    "            # Add recognized gesture to the stack\n",
    "            gesture_stack.append(recognized_gesture)\n",
    "\n",
    "            # Maintain the stack size to 20 elements\n",
    "            if len(gesture_stack) > 20:\n",
    "                gesture_stack.pop(0)\n",
    "            \n",
    "            if len(gesture_stack) == 20:\n",
    "                # Check for the majority recognized gesture\n",
    "                gesture_counts = {gesture: gesture_stack.count(gesture) for gesture in gesture_stack}\n",
    "                max_count = max(gesture_counts.values())\n",
    "                majority_gesture = [gesture for gesture, count in gesture_counts.items() if count == max_count]\n",
    "\n",
    "                # Check if the majority gesture exceeds the threshold\n",
    "                if (max_count / 20) * 100 >= threshold_gestures:\n",
    "                    assured_recognized_gesture = majority_gesture[0]\n",
    "                    pause_time = time.time()\n",
    "\n",
    "    # Display the frame with gesture overlay in the window\n",
    "    visualize_results(frame, gesture_results, VERBOSE)\n",
    "\n",
    "    # Exit the loop if 'q' key is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        # Release the webcam\n",
    "        cap.release() \n",
    "        # Exit all windows\n",
    "        cv2.destroyAllWindows()\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

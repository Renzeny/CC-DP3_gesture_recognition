{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project CC-DP3 \n",
    "\n",
    "In this notebook we will show how gesture control can be used for sending commands to an agent, the agent will eventually execute an order related to this gesture.\n",
    "\n",
    "In this example we use an external webcam to record the livestream of given gestures and will display an animation in real-time to confirm the action related to the gesture.\n",
    "\n",
    "The gestures are as follows: \n",
    "\n",
    "    - FOLLOW: üëÜ OR üëá\n",
    "    - STOP: ‚úã\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will download the model that will recognize the gestures - this model supports 7 hand gestures: üëç, üëé, ‚úåÔ∏è, ‚òùÔ∏è, ‚úä, üëã, ü§ü\n",
    "!wget -q https://storage.googleapis.com/mediapipe-models/gesture_recognizer/gesture_recognizer/float16/1/gesture_recognizer.task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BaseOptions = mp.tasks.BaseOptions\n",
    "GestureRecognizer = mp.tasks.vision.GestureRecognizer\n",
    "GestureRecognizerOptions = mp.tasks.vision.GestureRecognizerOptions\n",
    "GestureRecognizerResult = mp.tasks.vision.GestureRecognizerResult\n",
    "VisionRunningMode = mp.tasks.vision.RunningMode\n",
    "\n",
    "model_path = 'gesture_recognizer.task'\n",
    "base_options = BaseOptions(model_asset_path=model_path)\n",
    "\n",
    "# Create a gesture recognizer instance with the live stream mode:\n",
    "def print_result(result: GestureRecognizerResult, output_image: mp.Image, timestamp_ms: int):\n",
    "    print('gesture recognition result: {}'.format(result))\n",
    "\n",
    "options = GestureRecognizerOptions(\n",
    "    base_options=BaseOptions(model_asset_path='/path/to/model.task'),\n",
    "    running_mode=VisionRunningMode.LIVE_STREAM,\n",
    "    result_callback=print_result)\n",
    "with GestureRecognizer.create_from_options(options) as recognizer:\n",
    "  # The detector is initialized. Use it here.\n",
    "  # ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Use OpenCV‚Äôs VideoCapture to start capturing from the webcam.\n",
    "\n",
    "# Create a loop to read the latest frame from the camera using VideoCapture#read()\n",
    "\n",
    "# Convert the frame received from OpenCV to a MediaPipe‚Äôs Image object.\n",
    "mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=numpy_frame_from_opencv)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send live image data to perform gesture recognition.\n",
    "# The results are accessible via the `result_callback` provided in\n",
    "# the `GestureRecognizerOptions` object.\n",
    "# The gesture recognizer must be created with the live stream mode.\n",
    "recognizer.recognize_async(mp_image, frame_timestamp_ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Open the webcam (use 0 for the default camera)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Check if the webcam is opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open webcam.\")\n",
    "    exit()\n",
    "\n",
    "# Create a window to display the camera feed\n",
    "cv2.namedWindow(\"Webcam Feed\", cv2.WINDOW_NORMAL)\n",
    "\n",
    "while True:\n",
    "    # Read a frame from the webcam\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Check if the frame was read successfully\n",
    "    if not ret:\n",
    "        print(\"Error: Failed to grab frame.\")\n",
    "        break\n",
    "\n",
    "    # Display the frame in the window\n",
    "    cv2.imshow(\"Webcam Feed\", frame)\n",
    "\n",
    "    # Break the loop if 'q' key is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the webcam and close the window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
